{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step\n",
    "<img src='pic/nn_step.jpg' width='60%' height='60%'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape =  (5000, 400)\n",
      "y.shape =  (5000, 1)\n",
      "Theta1.shape =  (25, 401)\n",
      "Theta2.shape =  (10, 26)\n"
     ]
    }
   ],
   "source": [
    "# %load ../../../standard_import.txt\n",
    "import numpy as np\n",
    "# load MATLAB files\n",
    "from scipy.io import loadmat\n",
    "#%config InlineBackend.figure_formats = {'pdf',}\n",
    "%matplotlib inline\n",
    "\n",
    "data = loadmat('data/ex4data1.mat')\n",
    "X = data['X']\n",
    "y = data['y']%10 #这里将10还原为0\n",
    "# print 'y = ',np.unique(y)\n",
    "print 'X.shape = ',X.shape\n",
    "print 'y.shape = ',y.shape\n",
    "\n",
    "weights = loadmat('data/ex4weights.mat')\n",
    "weights.keys()\n",
    "Theta1 = weights['Theta1']\n",
    "Theta2 = weights['Theta2']\n",
    "print 'Theta1.shape = ',Theta1.shape\n",
    "print 'Theta2.shape = ',Theta2.shape\n",
    "# print np.r_[Theta1.ravel(),Theta2.ravel()].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function:</br>\n",
    "<img src='pic/nn_cost_fun.png' width='60%' height='60%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/sigmoid_grad.png' width='40%' height='40%'/>\n",
    "<img src='pic/nn_forword.png' width='40%' height='40%'/>\n",
    "后向推导结果：如果要看推导过程，可以查看：https://www.cnblogs.com/andywenzhi/p/7295262.html?utm_source=itdadao&utm_medium=referral </br>\n",
    "<img src='pic/nn_backword.png' width='40%' height='40%'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.441459672777979"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return(1.0 / (1 + np.exp(-z)))\n",
    "\n",
    "\n",
    "def sigmoid_grad(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "\n",
    "def nn_cost_fun(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, reg_lambda):\n",
    "    (m, n) = X.shape\n",
    "    # theta1 25*401\n",
    "    # theta2 10*26\n",
    "    Theta1 = nn_params[0:hidden_layer_size * (input_layer_size + 1)].reshape(hidden_layer_size, input_layer_size + 1)\n",
    "    Theta2 = nn_params[hidden_layer_size * (input_layer_size + 1):].reshape(num_labels, hidden_layer_size + 1)\n",
    "\n",
    "    Y = np.zeros([m, num_labels])\n",
    "    for i in range(m):\n",
    "        Y[i, y[i]] = 1\n",
    "        # Y[i, y[i]-1] = 1\n",
    "\n",
    "    # 前向计算\n",
    "    # a_1 5000*401\n",
    "    a_1 = np.c_[np.ones(m), X]\n",
    "\n",
    "    # z_2 5000*25\n",
    "    z_2 = a_1.dot(Theta1.T)\n",
    "    # a_2 5000*26\n",
    "    a_2 = np.c_[np.ones(m), sigmoid(z_2)]\n",
    "\n",
    "    # z_3 5000*10\n",
    "    z_3 = a_2.dot(Theta2.T)\n",
    "    a_3 = sigmoid(z_3)\n",
    "\n",
    "    # 注意正则化项并没有计算bias项\n",
    "    J = 1.0 / m * np.sum(-Y * np.log(a_3) - (1 - Y) * (np.log(1 - a_3)))\n",
    "    J_reg = J + (reg_lambda / (2.0 * m)) * (np.sum(np.square(Theta1[:, 1:])) + np.sum(np.square(Theta2[:, 1:])))\n",
    "\n",
    "    #     J = -1*(1/m)*np.sum((np.log(a3.T)*(y_matrix)+np.log(1-a3).T*(1-y_matrix))) + \\\n",
    "    #         (reg/(2*m))*(np.sum(np.square(theta1[:,1:])) + np.sum(np.square(theta2[:,1:])))\n",
    "\n",
    "\n",
    "    # 后向计算,对z求偏导数,记为delta\n",
    "    # delta_3 5000*10\n",
    "    delta_3 = a_3 - Y\n",
    "\n",
    "    # delta_2 5000*25\n",
    "    delta_2 = delta_3.dot(Theta2[:, 1:]) * sigmoid_grad(z_2)\n",
    "\n",
    "    # 后向计算,求参数的梯度,梯度的维度和thate一致\n",
    "    # 首先初始化梯度\n",
    "    # 10*26,25*401\n",
    "    d_2 = np.zeros(Theta2.shape)\n",
    "    d_1 = np.zeros(Theta1.shape)\n",
    "    # Δ(2)=Δ(2)+a(2)∗δ(3) 10*26\n",
    "    d_2 = d_2 + delta_3.T.dot(a_2)  # d_2初始化为0，可以不用初始化\n",
    "    # Δ(1)=Δ(1)+a(1)∗δ(2) 25*401\n",
    "    d_1 = d_1 + delta_2.T.dot(a_1)\n",
    "\n",
    "    Theta1_grad = d_1 / m + 1.0*reg_lambda/m*Theta1\n",
    "    Theta2_grad = d_2 / m + 1.0*reg_lambda/m*Theta2\n",
    "\n",
    "    theta_grad = np.r_[Theta1_grad.ravel(), Theta2_grad.ravel()]\n",
    "    # print 'J = ',J_reg\n",
    "    return (J_reg, theta_grad)\n",
    "\n",
    "init_params = np.r_[Theta1.ravel(),Theta2.ravel()]\n",
    "nn_cost_fun(init_params, 400, 25, 10, X, y, 0)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sigmoid gradient...\n",
      "Sigmoid gradient evaluated at [-1 -0.5 0 0.5 1]:\n",
      "[ 0.19661193  0.23500371  0.25        0.23500371  0.19661193]\n"
     ]
    }
   ],
   "source": [
    "# ================ Part 5: Sigmoid Gradient  ================\n",
    "print 'Evaluating sigmoid gradient...'\n",
    "g = sigmoid_grad(np.array([-1, -0.5, 0, 0.5, 1]))\n",
    "print 'Sigmoid gradient evaluated at [-1 -0.5 0 0.5 1]:'\n",
    "print g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ================ Part 6: Initializing Parameters Rand  ================\n",
    "\n",
    "\"\"\"\n",
    "1.One effective strategy for random initialization is to randomly select values for Θ(l) uniformly in the range [−ε, ε]. You should use ε = 0.12.\n",
    "2.This range of values ensures that the parameters are kept small and makes the learning more efficient.\n",
    "3.如果参数初始化为0，在bp计算过程中都一样了，这样将无意义\n",
    "\"\"\"\n",
    "def rand_initialize_weights(l_in, l_out):\n",
    "\n",
    "    \"\"\"\n",
    "    :param l_in:\n",
    "    :param l_out:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    epsilon_init = 0.12\n",
    "\n",
    "    return np.random.rand(l_out, l_in+1) * 2 * epsilon_init -epsilon_init\n",
    "\n",
    "rand_theta1 = rand_initialize_weights(400, 25)\n",
    "rand_theta2 = rand_initialize_weights(25, 10)\n",
    "rand_params = np.r_[rand_theta1.ravel(), rand_theta2.ravel()]\n",
    "\n",
    "# print nn_cost_fun(rand_params, 400, 25, 10, X, y, 0)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### ================ Part 7: compute_numerical_gradient  ================\n",
    "1.使用数值方法对梯度进行检测，检测bp算法是否正确，方法就是对每一个参数值进行小的改变，然后计算值，用数学的方法求梯度<br>\n",
    "2.每修改一个参数，就需要重新跑前向，后向算法来得到这个参数的梯度，所以效率非常慢，实际跑过程中尽量关闭<br>\n",
    "3.If your backpropagation implementation is correct, then the relative difference will be small (less than 1e-9) Relative Difference\n",
    "\n",
    "<img src='pic/number_grad_check.png' width='60%' height='60%'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuancailei/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:22: RuntimeWarning: invalid value encountered in divide\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  4.31032895e-10,              nan,              nan, ...,\n",
       "         7.39688649e-11,   1.29840220e-10,   5.91227778e-11])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_numerical_grad(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, reg_lambda, h=0.00001):\n",
    "    params_cnt = nn_params.shape[0]\n",
    "    param_gradient = np.zeros(params_cnt)\n",
    "    for i in range(params_cnt) :\n",
    "        nn_params_temp = np.copy(nn_params)\n",
    "        nn_params_temp[i] = nn_params[i] + h\n",
    "        j_1 = nn_cost_fun(nn_params_temp, input_layer_size, hidden_layer_size, num_labels, X, y, reg_lambda)[0]\n",
    "        nn_params_temp = np.copy(nn_params)\n",
    "        nn_params_temp[i] = nn_params[i] - h\n",
    "        j_2 = nn_cost_fun(nn_params_temp, input_layer_size, hidden_layer_size, num_labels, X, y, reg_lambda)[0]\n",
    "\n",
    "        param_gradient[i] = (j_1 - j_2)/(2*h)\n",
    "\n",
    "    return param_gradient\n",
    "\n",
    "#测试,选择5个样本\n",
    "sample = np.random.choice(X.shape[0], 5)\n",
    "XX = X[sample]\n",
    "yy = y[sample]\n",
    "nn_param_grad = nn_cost_fun(rand_params, 400, 25, 10, XX, yy, 0)[1]\n",
    "number_param_grad = compute_numerical_grad(rand_params, 400, 25, 10, XX, yy, 0)\n",
    "diff = np.abs(number_param_grad-nn_param_grad)/(np.abs(number_param_grad)+np.abs(nn_param_grad))\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 98.8%\n"
     ]
    }
   ],
   "source": [
    "### ================ Part 8: Training Neural Network And Predict  ================\n",
    "import scipy.optimize as opt\n",
    "\n",
    "input_layer_size = 400   # 20x20 Input Images of Digits\n",
    "hidden_layer_size = 25   # 25 hidden units\n",
    "num_labels = 10\n",
    "\n",
    "def predict(Theta_1, Theta_2, X):\n",
    "    \n",
    "    m, n = X.shape\n",
    "    # 前向计算\n",
    "    # a_1 5000*401\n",
    "    a_1 = np.c_[np.ones(m), X]\n",
    "\n",
    "    # z_2 5000*25\n",
    "    z_2 = a_1.dot(Theta_1.T)\n",
    "    # a_2 5000*26\n",
    "    a_2 = np.c_[np.ones(m), sigmoid(z_2)]\n",
    "\n",
    "    # z_3 5000*10\n",
    "    z_3 = a_2.dot(Theta_2.T)\n",
    "    a_3 = sigmoid(z_3)\n",
    "\n",
    "    return np.argmax(a_3, axis=1)\n",
    "\n",
    "\n",
    "l = 1\n",
    "result = opt.minimize(fun=nn_cost_fun, x0=rand_params,\n",
    "                      args=(input_layer_size, hidden_layer_size, num_labels, X, y, l),\n",
    "                      method='TNC', jac=True, options={'maxiter': 150})\n",
    "params_trained = result.x\n",
    "\n",
    "Theta_1_trained = params_trained[0:hidden_layer_size * (input_layer_size + 1)].reshape(hidden_layer_size, input_layer_size + 1)\n",
    "Theta_2_trained = params_trained[hidden_layer_size * (input_layer_size + 1):].reshape(num_labels, hidden_layer_size + 1)\n",
    "\n",
    "# print 'Visualizing Neural Network...'\n",
    "# plt.figure()\n",
    "# display_data(Theta_1_trained[:, 1:], padding=1)\n",
    "# plt.show()\n",
    "\n",
    "# =================Implement Predict =================\n",
    "pred = predict(Theta_1_trained, Theta_2_trained, X)\n",
    "# print 'Training Set Accuracy:', np.mean(pred == y) * 100\n",
    "print 'accuracy is {}%'.format(np.mean(pred == y.ravel())*100)\n",
    "\n",
    "# print 'pred front 10 is : ', pred[3800:4050]\n",
    "# print '   y front 10 is : ', y.ravel()[3800:4050]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
