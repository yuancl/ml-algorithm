### 理解矩阵的本质，特征向量，特征值
https://jingyan.baidu.com/article/3065b3b68c6bb6becff8a488.html <br>
#### 1.一个矩阵其实就是一个线性变换
#### 2.线性变换有很多方向，描述好一个变换，那么只需要描述好这个变换的主要的变化方向就好
这其实是在平面上对一个轴进行的拉伸变换（如蓝色的箭头所示），在图中，蓝色的箭头是一个**最主要的变化方向**（变化方向可能有不止一个），如果我们想要描述好一个变换，那我们就描述好这个**变换主要的变化方向**就好了
#### 3.一个矩阵在高维空间下，这个变换也同样有很多的变换方向，我们通过特征值分解得到的前N个特征向量，那么就对应了这个矩阵最主要的N个变化方向。我们利用这前N个变化方向，就可以近似这个矩阵（变换）
总结一下，特征值分解可以得到特征值与特征向量，特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么，可以将每一个特征向量理解为一个线性的子空间，我们可以利用这些线性的子空间干很多的事情
#### 4.PCA算法对输入数据具有缩放不变性，无论输入数据的值被如何放大（或缩小），返回的特征向量都不改变


### 理解PCA
官方教程：http://deeplearning.stanford.edu/wiki/index.php/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90 <br>
PCA理论可以参考我之前写的一篇简书：https://www.jianshu.com/p/6a70d95c5d8b <br>
如果对上面说的矩阵的本质，特征向量，特征值有了比较好的理解，**那么PCA就很容易理解了** <br>
#### 1.通过计算数据集的协方差矩阵 <br>
#### 2.计算协方差矩阵的特征向量和特征值，即获得主要变化的方向，即主成分 <br>
#### 3.计算原始数据在特征向量上的映射，即投影,也可以说为旋转，变换 <br>


np.linalg.svd 方法对矩阵的特征值分解，A = Q*sigma*Q.T


疑问：
Part 5: PCA on Face Data: Eigenfaces，这个部分中，把特征向量化出来，没有理解含义