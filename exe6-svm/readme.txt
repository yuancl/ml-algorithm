个人感觉老师是视频和资源对svm的讲解是直切入主题，讲的是重点。但是对svm的一些基本概念和推导并没有做一些介绍，
建议可以结合西瓜书来学习svm,恩达老师介绍的部分，使用的sklearn的svm进行测验，主要是理解svm的核心概念，并没有对算法进行实现，
对算法实现感兴趣的可以学习西瓜书svm这一章节


当数据为非线性可分的时候，当然可以采用之前学习的多项式的方式进行特征的扩充，但是特征如何选择，几次方的是比较难抉择的。这就
出现的svm的kernel概念

高斯kernel原理：
将低维坐标映射为高维坐标，在高维空间找到切面进行数据的划分，当然这个数据证明过程可以查看其他资料。
高斯三维图可以看见越靠近landmark的点，映射值就越接近1，远离landmrk的点，映射值越接近0
步骤：
1.确定landmarks，ppt上有介绍，一般会将所有点都处理为landmarks
2.确定方差值gamma，此值越大，越扁平，当然后面会采用多组值的方式选择最优的方差值
3.计算相似值，也就是将每个x值，和1只能够确定的每一个landmark映射为一个新的高维特征，这样就会映射为m个features
4.然后进行迭代优化，其中判断Theta(T)f(x)>0 是否大于0来判断输出y为0还是1


重要参数的选择：
1.C，这里的c是充当1/lambda:正则化参数的作用
2.gamma，此值越大，越扁平，当然后面会采用多组值的方式选择最优的方差值
3.landmarks点，ppt上有介绍，一般会将所有点都处理为landmarks(有多少landmark点，就会新增多少特征值)


老师的这个视频对svm的原理的讲解还是浅显，很多原理都理解得都不是很透彻，留下了很多疑问，需要进一步参考西瓜书等(比如使用对偶算法，和拉格朗日求最优解等)
1.映射到高维空间，为什么就保证一定可分了
2.高维空间中的，Theta0+Theta1*f(x1)+Theta2*f(x2)+....... > 0 就分类出y=1等，为什么不是 > 1
